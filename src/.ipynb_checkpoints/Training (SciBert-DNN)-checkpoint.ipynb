{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* python >= 3.7\n",
    "* pytorch >= 1.0\n",
    "* pandas\n",
    "* nltk\n",
    "* numpy\n",
    "* sklearn\n",
    "* pickle\n",
    "* tqdm\n",
    "* json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ipywidgets as widgets\n",
    "torch.manual_seed(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a  view of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "dataset = pd.read_csv('../data/task2_trainset.csv', dtype=str)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Id**: 流水號  \n",
    "**Title**: 論文標題  \n",
    "**Abstract**: 論文摘要內容, 句子間以 **$$$** 分隔  \n",
    "**Authors**: 論文作者  \n",
    "**Categories**: 論文類別  \n",
    "**Created date**: 論文上傳日期  \n",
    "**Task 2**: 論文分類類別, 若句子有多個類別,以 **空格** 分隔 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 刪除多於資訊 (Remove redundant information)  \n",
    "我們在資料集中保留了許多額外資訊供大家使用，但是在這次的教學中我們並沒有用到全部資訊，因此先將多餘的部分先抽走。  \n",
    "In dataset, we reserved lots of information. But in this tutorial, we don't need them, so we need to discard them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop('Categories',axis=1,inplace=True)\n",
    "dataset.drop('Created Date',axis=1, inplace=True)\n",
    "dataset.drop('Authors',axis=1,inplace=True)\n",
    "dataset.drop('CiteEmbed',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料切割  (Partition)\n",
    "在訓練時，我們需要有個方法去檢驗訓練結果的好壞，因此需要將訓練資料切成training/validataion set。   \n",
    "While training, we need some method to exam our model's performance, so we divide our training data into training/validataion set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_all = dataset\n",
    "trainset, validset = train_test_split(dataset, test_size=0.1, random_state=17)\n",
    "\n",
    "print (len(train_all))\n",
    "print (len(trainset))\n",
    "print (len(validset))\n",
    "\n",
    "train_all.to_csv('trainallset.csv', index=False)\n",
    "trainset.to_csv('trainset.csv', index=False)\n",
    "validset.to_csv('validset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/task2_public_testset.csv', dtype=str)\n",
    "#dataset.drop('Title',axis=1,inplace=True)\n",
    "dataset.drop('Categories',axis=1,inplace=True)\n",
    "dataset.drop('Created Date',axis=1, inplace=True)\n",
    "dataset.drop('Authors',axis=1,inplace=True)\n",
    "dataset.drop('CiteEmbed',axis=1,inplace=True)\n",
    "dataset.to_csv('testset.csv',index=False)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料格式化 (Data formatting)  \n",
    "有了字典後，接下來我們要把資料整理成一筆一筆，把input的句子轉成數字，把答案轉成onehot的形式。  \n",
    "這裡，我們一樣使用`multiprocessing`來加入進行。  \n",
    "After building dictionary, that's mapping our sentences into number array, and convert answers to onehot format.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For BERT\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def label_to_onehot(labels):\n",
    "    \"\"\" Convert label to onehot .\n",
    "        Args:\n",
    "            labels (string): sentence's labels.\n",
    "        Return:\n",
    "            outputs (onehot list): sentence's onehot label.\n",
    "    \"\"\"\n",
    "    label_dict = {'THEORETICAL': 0, 'ENGINEERING':1, 'EMPIRICAL':2, 'OTHERS':3}\n",
    "    onehot = [0,0,0,0]\n",
    "    for l in labels.split():\n",
    "        onehot[label_dict[l]] = 1\n",
    "    return onehot\n",
    "    \n",
    "def get_dataset(data_path, n_workers=4):\n",
    "    \"\"\" Load data and return dataset for training and validating.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the data.\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(data_path, dtype=str)\n",
    "\n",
    "    results = [None] * n_workers\n",
    "    with Pool(processes=n_workers) as pool:\n",
    "        for i in range(n_workers):\n",
    "            batch_start = (len(dataset) // n_workers) * i\n",
    "            if i == n_workers - 1:\n",
    "                batch_end = len(dataset)\n",
    "            else:\n",
    "                batch_end = (len(dataset) // n_workers) * (i + 1)\n",
    "            \n",
    "            batch = dataset[batch_start: batch_end]\n",
    "            results[i] = pool.apply_async(preprocess_samples, args=(batch,))\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    processed = []\n",
    "    for result in results:\n",
    "        processed += result.get()\n",
    "    return processed\n",
    "\n",
    "def preprocess_samples(dataset):\n",
    "    \"\"\" Worker function.\n",
    "\n",
    "    Args:\n",
    "        dataset (list of dict)\n",
    "    Returns:\n",
    "        list of processed dict.\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "    for sample in tqdm(dataset.iterrows(), total=len(dataset)):\n",
    "        processed.append(preprocess_sample(sample[1]))\n",
    "\n",
    "    return processed\n",
    "\n",
    "def preprocess_sample(data):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (dict)\n",
    "    Returns:\n",
    "        dict\n",
    "    \"\"\"\n",
    "    processed = {}\n",
    "    processed['Abstract'] = [data['Title'] + \".\"] + [sent for sent in data['Abstract'].split('$$$')]\n",
    "    #processed['Abstract'] = [sent for sent in data['Abstract'].split('$$$')]\n",
    "    #print (processed['Abstract'])\n",
    "    if 'Task 2' in data:\n",
    "        processed['Label'] = label_to_onehot(data['Task 2'])\n",
    "        \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('[INFO] Start processing trainallset...')\n",
    "trainall = get_dataset('trainallset.csv', n_workers=4)\n",
    "print('[INFO] Start processing trainset...')\n",
    "#train = get_dataset('trainset.csv', embedder, n_workers=4)\n",
    "train = get_dataset('trainset.csv', n_workers=4)\n",
    "print('[INFO] Start processing validset...')\n",
    "#valid = get_dataset('validset.csv', embedder, n_workers=4)\n",
    "valid = get_dataset('validset.csv', n_workers=4)\n",
    "print('[INFO] Start processing testset...')\n",
    "#test = get_dataset('testset.csv', embedder, n_workers=4)\n",
    "test = get_dataset('testset.csv', n_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料封裝 (Data packing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "為了更方便的進行batch training，我們將會借助[torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)。  \n",
    "而要將資料放入dataloader，我們需要繼承[torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)，撰寫適合這份dataset的class。  \n",
    "`collate_fn`用於batch data的後處理，在`dataloder`將選出的data放進list後會呼叫collate_fn，而我們會在此把sentence padding到同樣的長度，才能夠放入torch tensor (tensor必須為矩陣)。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To easily training in batch, we'll use `dataloader`, which is a function built in Pytorch[torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)  \n",
    "To use datalaoder, we need to packing our data into class `dataset` [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)  \n",
    "`collate_fn` is used for data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from pytorch_transformers import *\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print (device)\n",
    "#bert-large-cased-whole-word-masking-finetuned-squad 1024\n",
    "#bert-base-uncased 768 \n",
    "#bert-large-uncased 1024\n",
    "#./scibert-scivocab-uncased/\n",
    "tokenizer = BertTokenizer.from_pretrained('../scibert_scivocab_uncased/')\n",
    "bert_model = BertModel.from_pretrained('../scibert_scivocab_uncased/')\n",
    "bert_model.to(device)\n",
    "bert_model.train(False)\n",
    "bert_model.eval()\n",
    "\n",
    "class BertDataset(Dataset):\n",
    "    def __init__(self, data, max_len = 256, n_workers=1):        \n",
    "        processed_data = []\n",
    "        for d in tqdm(data, total=len(data)):\n",
    "            processed_d = {'Abstract':[], 'Label':[]}\n",
    "            token_sent = None\n",
    "            for idx, sentence in enumerate(d['Abstract']):\n",
    "                if idx==0:\n",
    "                    token_sent = tokenizer.convert_tokens_to_ids(['[CLS]'] + tokenizer.tokenize(sentence) + ['[SEP]'])\n",
    "                else: \n",
    "                    token_sent += tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence) + ['[SEP]'])\n",
    "            if (len(token_sent) > 511):\n",
    "                token_sent = token_sent[:511] + tokenizer.convert_tokens_to_ids(['[SEP]'])\n",
    "            #print (len(token_sent))\n",
    "            encode_sentence_tensor = torch.tensor([token_sent])\n",
    "            encode_sentence_tensor = encode_sentence_tensor.to(device)\n",
    "            with torch.no_grad():\n",
    "                out = bert_model(encode_sentence_tensor)[0]\n",
    "                #print (out.shape)\n",
    "                out = out[:,-1,:]\n",
    "                #print (out.shape)\n",
    "            #processed_d['Abstract'] = [list(itertools.chain(out.to('cpu').tolist()[0], DocEmbed_32[d['PaperId']], DocEmbed_64[d['PaperId']], DocEmbed_128[d['PaperId']]))]\n",
    "            #processed_d['Abstract'] = [list(itertools.chain(out.to('cpu').tolist()[0], DocEmbed_64[d['PaperId']]))]\n",
    "            #print(DocEmbed_title_32[d['PaperId']].tolist())\n",
    "#             processed_d['Abstract'] = [list(itertools.chain(out.to('cpu').tolist()[0], DocEmbed_title_32[d['PaperId']].tolist(), CateEmbed_64[d['PaperId']].tolist()))]\n",
    "            processed_d['Abstract'] = [list(itertools.chain(out.to('cpu').tolist()[0]))]\n",
    "            #print(len(processed_d['Abstract'][0]))\n",
    "            #processed_d['Abstract'] = [out.to('cpu').tolist()[0]]\n",
    "            if 'Label' in d:\n",
    "                processed_d['Label'] = d['Label']\n",
    "            processed_data.append(processed_d)\n",
    "        \n",
    "        self.data = processed_data\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) # return data筆數\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "     \n",
    "    \n",
    "    def collate_fn(self, datas):\n",
    "        # get max length in this batch\n",
    "        max_sent = max([len(d['Abstract']) for d in datas])# Get max length of sentence in datas\n",
    "        batch_abstract = None\n",
    "        batch_label = []\n",
    "        for idx, data in enumerate(datas):\n",
    "            # padding abstract to make them in same length\n",
    "            #pad_abstract = data['Abstract']\n",
    "            if idx==0:\n",
    "                batch_abstract = data['Abstract']\n",
    "            else: \n",
    "                batch_abstract += data['Abstract']\n",
    "            #print (len(batch_abstract))\n",
    "            # gather labels\n",
    "            if 'Label' in data:\n",
    "                batch_label.append(data['Label'])\n",
    "        #print (batch_abstract)\n",
    "        #print (batch_label)\n",
    "        return torch.FloatTensor(batch_abstract), torch.FloatTensor(batch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trainallData = BertDataset(trainall)\n",
    "\n",
    "# trainData = BertDataset(train)\n",
    "\n",
    "# validData = BertDataset(valid)\n",
    "\n",
    "testData = BertDataset(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "資料處理完成後，接下來就是最重要的核心部分：`Model`。  \n",
    "此次範例中我們以簡單的一層RNN + 一層Linear layer作為示範。  \n",
    "而為了解決每次的句子長度不一的問題(`linear layer必須是fixed input size`)，因此我們把所有字的hidden_state做平均，讓這一個vector代表這句話。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we're going to implement a simple model, which contain one RNN layer and one fully connected layers (Linear layer). Of course you can make it \"deep\".  \n",
    "To solve variant sentence length problem (`input size in linear layer must be fixed`), we can average all hidden_states, and become one vector. (Perfect!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class simpleNet(nn.Module):\n",
    "    def __init__(self, vocabulary_size):\n",
    "        super(simpleNet, self).__init__()\n",
    "        self.hidden_dim1 = 512\n",
    "        self.l0 = nn.Linear((vocabulary_size), (vocabulary_size))\n",
    "        self.b0 = nn.Parameter(torch.zeros(vocabulary_size))\n",
    "        self.l1 = nn.Linear((vocabulary_size), self.hidden_dim1)\n",
    "        self.b1 = nn.Parameter(torch.zeros(self.hidden_dim1))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.l2 = nn.Linear(self.hidden_dim1, 4)\n",
    "        self.b2 = nn.Parameter(torch.zeros(4))\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b,e = x.shape\n",
    "        x0 = self.relu1(self.l0(x)+self.b0)\n",
    "        x0 = self.dropout(x0)\n",
    "        x = self.relu1(self.l1(x+x0)+self.b1)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.l2(x)+self.b2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regularization(torch.nn.Module):\n",
    "    def __init__(self,model,weight_decay,p=2):\n",
    "        '''\n",
    "        :param model 模型\n",
    "        :param weight_decay:正则化参数\n",
    "        :param p: 范数计算中的幂指数值，默认求2范数,\n",
    "                  当p=0为L2正则化,p=1为L1正则化\n",
    "        '''\n",
    "        super(Regularization, self).__init__()\n",
    "        if weight_decay <= 0:\n",
    "            print(\"param weight_decay can not <=0\")\n",
    "            exit(0)\n",
    "        self.model=model\n",
    "        self.weight_decay=weight_decay\n",
    "        self.p=p\n",
    "        self.weight_list=self.get_weight(model)\n",
    "        self.weight_info(self.weight_list)\n",
    " \n",
    "    def to(self,device):\n",
    "        '''\n",
    "        指定运行模式\n",
    "        :param device: cude or cpu\n",
    "        :return:\n",
    "        '''\n",
    "        self.device=device\n",
    "        super().to(device)\n",
    "        return self\n",
    " \n",
    "    def forward(self, model):\n",
    "        self.weight_list=self.get_weight(model)#获得最新的权重\n",
    "        reg_loss = self.regularization_loss(self.weight_list, self.weight_decay, p=self.p)\n",
    "        return reg_loss\n",
    " \n",
    "    def get_weight(self,model):\n",
    "        '''\n",
    "        获得模型的权重列表\n",
    "        :param model:\n",
    "        :return:\n",
    "        '''\n",
    "        weight_list = []\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                weight = (name, param)\n",
    "                weight_list.append(weight)\n",
    "        return weight_list\n",
    " \n",
    "    def regularization_loss(self,weight_list, weight_decay, p=2):\n",
    "        '''\n",
    "        计算张量范数\n",
    "        :param weight_list:\n",
    "        :param p: 范数计算中的幂指数值，默认求2范数\n",
    "        :param weight_decay:\n",
    "        :return:\n",
    "        '''\n",
    "        # weight_decay=Variable(torch.FloatTensor([weight_decay]).to(self.device),requires_grad=True)\n",
    "        # reg_loss=Variable(torch.FloatTensor([0.]).to(self.device),requires_grad=True)\n",
    "        # weight_decay=torch.FloatTensor([weight_decay]).to(self.device)\n",
    "        # reg_loss=torch.FloatTensor([0.]).to(self.device)\n",
    "        reg_loss=0\n",
    "        for name, w in weight_list:\n",
    "            l2_reg = torch.norm(w, p=p)\n",
    "            reg_loss = reg_loss + l2_reg\n",
    " \n",
    "        reg_loss=weight_decay*reg_loss\n",
    "        return reg_loss\n",
    " \n",
    "    def weight_info(self,weight_list):\n",
    "        '''\n",
    "        打印权重列表信息\n",
    "        :param weight_list:\n",
    "        :return:\n",
    "        '''\n",
    "        print(\"---------------regularization weight---------------\")\n",
    "        for name ,w in weight_list:\n",
    "            print(name)\n",
    "        print(\"---------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指定使用的運算裝置  \n",
    "Designate running device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定義一個算分公式, 讓我們在training能快速了解model的效能  \n",
    "Define score function, let us easily observe model performance while training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "class F1():\n",
    "    def __init__(self):\n",
    "        self.threshold = torch.tensor([0.5,0.5,0.5])\n",
    "        self.n_precision = 0\n",
    "        self.n_recall = 0\n",
    "        self.n_corrects = 0\n",
    "        self.name = 'F1'\n",
    "        \n",
    "    def extend_label(self, x):\n",
    "        idx = torch.where(torch.sum(x[:,:3],1) == 0)[0]\n",
    "        y = torch.zeros((x.shape[0],4))\n",
    "        y[:,:3] = x[:,:3]\n",
    "        y[idx,3] = 1\n",
    "        return y\n",
    "        \n",
    "    def search_best_f1(self, y_pred, y_true):\n",
    "        pred = y_pred.clone().detach()\n",
    "        value = 0\n",
    "        for i in itertools.product(list(np.arange(350, 650, 10)), repeat = 3):\n",
    "            y_pred = pred.clone().detach()\n",
    "            #print (i)\n",
    "            #print (y_pred)\n",
    "            \n",
    "            for j in range(len(i)):\n",
    "                y_pred[:,j] = y_pred[:,j] > i[j]*0.001\n",
    "            #print (y_pred)\n",
    "            n_precision = torch.sum(y_pred)\n",
    "            n_recall = torch.sum(y_true)\n",
    "            n_corrects = torch.sum(y_pred*y_true)\n",
    "            recall = n_corrects/n_recall\n",
    "            precision = n_corrects/(n_precision + 1e-20)\n",
    "            if(value < 2 * (recall * precision) / (recall + precision + 1e-20)):\n",
    "                value = 2 * (recall * precision) / (recall + precision + 1e-20)\n",
    "                best_weight = [int(x)*0.001 for x in list(i)]\n",
    "                \n",
    "        #print(best_weight, value)\n",
    "        self.threshold = torch.tensor(best_weight)\n",
    "    \n",
    "    def get_threshold(self):\n",
    "        return self.threshold\n",
    "    \n",
    "    def set_threshold(self, thres):\n",
    "        self.threshold = thres\n",
    "\n",
    "    def reset(self):\n",
    "        self.n_precision = 0\n",
    "        self.n_recall = 0\n",
    "        self.n_corrects = 0\n",
    "\n",
    "    def update(self, predicts, groundTruth):\n",
    "        predicts[:,:3] = predicts[:,:3] > self.threshold\n",
    "        predicts = self.extend_label(predicts)\n",
    "        self.n_precision += torch.sum(predicts).data.item()\n",
    "        self.n_recall += torch.sum(groundTruth).data.item()\n",
    "        self.n_corrects += torch.sum(groundTruth.type(torch.uint8) * predicts.type(torch.uint8)).data.item()\n",
    "\n",
    "    def get_score(self):\n",
    "        recall = self.n_corrects / self.n_recall\n",
    "        precision = self.n_corrects / (self.n_precision + 1e-20)\n",
    "        return 2 * (recall * precision) / (recall + precision + 1e-20)\n",
    "\n",
    "    def print_score(self):\n",
    "        score = self.get_score()\n",
    "        return '{:.5f}'.format(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def ExtendLabel(x):\n",
    "    idx = torch.where(torch.sum(x[:,:3],1) == 0)[0]\n",
    "    y = torch.zeros((x.shape[0],4))\n",
    "    y[:,:3] = x[:,:3]\n",
    "    y[idx,3] = 1\n",
    "    return y\n",
    "\n",
    "def _run_epoch(epoch, training, thres=None):\n",
    "    model.train(training)\n",
    "    if training:\n",
    "        description = 'Train'\n",
    "        dataset = trainData\n",
    "        shuffle = True\n",
    "    else:\n",
    "        description = 'Valid'\n",
    "        dataset = validData\n",
    "        shuffle = False\n",
    "    dataloader = DataLoader(dataset=dataset,\n",
    "                            batch_size=8,\n",
    "                            shuffle=shuffle,\n",
    "                            collate_fn=dataset.collate_fn,\n",
    "                            num_workers=8)\n",
    "\n",
    "    trange = tqdm(enumerate(dataloader), total=len(dataloader), desc=description)\n",
    "    loss = 0\n",
    "    f1_score = F1()\n",
    "    labels_all = None\n",
    "    ys = None\n",
    "    for i, (x, y) in trange:\n",
    "        #print (x.shape)\n",
    "        o_labels, batch_loss = _run_iter(x,y)\n",
    "        if training:\n",
    "            opt.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        loss += batch_loss.item()\n",
    "        o_labels = o_labels.cpu()\n",
    "        \n",
    "        if (i == 0):\n",
    "            labels_all=o_labels\n",
    "            ys=y\n",
    "        else:\n",
    "            labels_all = torch.cat((labels_all, o_labels),dim=0)\n",
    "            ys = torch.cat((ys, y),dim=0)\n",
    "            \n",
    "        f1_score.update(o_labels, y)\n",
    "\n",
    "        trange.set_postfix(\n",
    "            loss=loss / (i + 1), f1=f1_score.print_score())\n",
    "    if training:        \n",
    "        print ('w/o threshold, training F1:{0:.5f}, training Loss:{1:.5f}'.format(f1_score.get_score(),float(loss/ len(trange))))\n",
    "        history['train'].append({'f1':f1_score.get_score(), 'loss':loss/ len(trange)})\n",
    "        f1_score.reset()\n",
    "        f1_score.search_best_f1(labels_all[:,:3], ys[:,:3])\n",
    "        f1_score.update(labels_all, ys)\n",
    "        print(f1_score.threshold)\n",
    "        print ('w/  threshold, training F1:{0:.5f}, training Loss:{1:.5f}'.format(f1_score.get_score(),float(loss/ len(trange))))\n",
    "        return f1_score.threshold\n",
    "    else:        \n",
    "        print ('w/o threshold, validating F1:{0:.5f}, validating Loss:{1:.5f}'.format(f1_score.get_score(),float(loss/ len(trange))))\n",
    "        history['valid'].append({'f1':f1_score.get_score(), 'loss':loss/ len(trange)})\n",
    "        f1_score.reset()\n",
    "        f1_score.set_threshold(thres)\n",
    "        f1_score.update(labels_all, ys)\n",
    "        print ('w/  threshold, validating F1:{0:.5f}, validating Loss:{1:.5f}'.format(f1_score.get_score(),float(loss/ len(trange))))\n",
    "        return f1_score.get_score()\n",
    "        \n",
    "def _run_iter(x,y):\n",
    "    abstract = x.to(device)\n",
    "    labels = y.to(device)\n",
    "    o_labels = model(abstract)\n",
    "    l_loss = criteria(o_labels, labels)\n",
    "#     +0.1*F1_loss(o_labels, labels)\n",
    "#     l_loss = F1_loss(o_labels, labels)\n",
    "    \n",
    "    if weight_decay > 0:\n",
    "        loss = l_loss + reg_loss(model)\n",
    "    total_loss = loss\n",
    "    return o_labels, total_loss\n",
    "    \n",
    "    return o_labels, l_loss\n",
    "\n",
    "def save(epoch):\n",
    "    if not os.path.exists('model'):\n",
    "        os.makedirs('model')\n",
    "    torch.save(model.state_dict(), 'model/model.pkl.'+str(epoch))\n",
    "    with open('model/history.json', 'w') as f:\n",
    "        json.dump(history, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SubmitGenerator(prediction, sampleFile, public=True, filename='prediction.csv'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        prediction (numpy array)\n",
    "        sampleFile (str)\n",
    "        public (boolean)\n",
    "        filename (str)\n",
    "    \"\"\"\n",
    "    sample = pd.read_csv(sampleFile)\n",
    "    submit = {}\n",
    "    submit['order_id'] = list(sample.order_id.values)\n",
    "    redundant = len(sample) - prediction.shape[0]\n",
    "    if public:\n",
    "        submit['THEORETICAL'] = list(prediction[:,0]) + [0]*redundant\n",
    "        submit['ENGINEERING'] = list(prediction[:,1]) + [0]*redundant\n",
    "        submit['EMPIRICAL'] = list(prediction[:,2]) + [0]*redundant\n",
    "        submit['OTHERS'] = list(prediction[:,3]) + [0]*redundant\n",
    "    else:\n",
    "        submit['THEORETICAL'] = [0]*redundant + list(prediction[:,0])\n",
    "        submit['ENGINEERING'] = [0]*redundant + list(prediction[:,1])\n",
    "        submit['EMPIRICAL'] = [0]*redundant + list(prediction[:,2])\n",
    "        submit['OTHERS'] = [0]*redundant + list(prediction[:,3])\n",
    "    df = pd.DataFrame.from_dict(submit) \n",
    "    df.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_loss(probas, target, epsilon=1e-7):\n",
    "    TP = (probas * target).sum(dim=1)\n",
    "    precision = TP / (probas.sum(dim=1) + epsilon)\n",
    "    recall = TP / (target.sum(dim=1) + epsilon)\n",
    "    f1 = 2 * precision * recall / (precision + recall + epsilon)\n",
    "    f1 = f1.clamp(min=epsilon, max=1-epsilon)\n",
    "    return 1 - f1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=50)\n",
    "for train_index, valid_index in kf.split(trainall):\n",
    "    model = simpleNet(768)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=9e-6)\n",
    "    criteria = torch.nn.BCELoss()\n",
    "\n",
    "    weight_decay=0.015 # 正则化参数\n",
    "    model.to(device)\n",
    "\n",
    "    # 初始化正则化\n",
    "    if weight_decay>0:\n",
    "        reg_loss=Regularization(model, weight_decay, p=2).to(device)\n",
    "    else:\n",
    "        print(\"no regularization\")\n",
    "    \n",
    "    train = [trainall[i] for i in train_index]\n",
    "    valid = [trainall[i] for i in valid_index]\n",
    "    trainData = BertDataset(train)\n",
    "    validData = BertDataset(valid)\n",
    "    \n",
    "    max_epoch = 120\n",
    "    history = {'train':[],'valid':[]}\n",
    "\n",
    "    thres_all = []\n",
    "\n",
    "    #print (embedder.get_vocabulary_size())\n",
    "    #print (embedder.get_dim())\n",
    "    #embedding = nn.Embedding(embedder.get_vocabulary_size(),embedder.get_dim())\n",
    "    #embedding.weight = torch.nn.Parameter(embedder.vectors)\n",
    "    \n",
    "    for epoch in range(max_epoch):\n",
    "        print('Epoch: {}'.format(epoch))\n",
    "        thres = _run_epoch(epoch, True)\n",
    "        _run_epoch(epoch, False, thres=thres)\n",
    "        thres_all.append(thres)\n",
    "        save(epoch)\n",
    "\n",
    "    %matplotlib inline\n",
    "\n",
    "    with open('model/history.json', 'r') as f:\n",
    "        history = json.loads(f.read())\n",
    "\n",
    "    train_loss = [l['loss'] for l in history['train']]\n",
    "    valid_loss = [l['loss'] for l in history['valid']]\n",
    "    train_f1 = [l['f1'] for l in history['train']]\n",
    "    valid_f1 = [l['f1'] for l in history['valid']]\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.title('Loss')\n",
    "    plt.plot(train_loss, label='train')\n",
    "    plt.plot(valid_loss, label='valid')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.title('F1 Score')\n",
    "    plt.plot(train_f1, label='train')\n",
    "    plt.plot(valid_f1, label='valid')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print('Best F1 score ', max([[l['f1'], idx] for idx, l in enumerate(history['valid'])]))\n",
    "    model_number = max([[l['f1'], idx] for idx, l in enumerate(history['valid'])])[1]\n",
    "\n",
    "    model.load_state_dict(torch.load('model/model.pkl.{}'.format(model_number)))\n",
    "\n",
    "    valid_value = _run_epoch(epoch, False, thres=thres_all[model_number])\n",
    "\n",
    "    print (thres_all[model_number])\n",
    "    model.train(False)\n",
    "    dataloader = DataLoader(dataset=testData,\n",
    "                                batch_size=64,\n",
    "                                shuffle=False,\n",
    "                                collate_fn=testData.collate_fn,\n",
    "                                num_workers=4)\n",
    "    trange = tqdm(enumerate(dataloader), total=len(dataloader), desc='Predict')\n",
    "    prediction = []\n",
    "    result = []\n",
    "    for i, (x,y) in trange:\n",
    "        o_labels = model(x.to(device))\n",
    "        o_labels = o_labels.to('cpu')\n",
    "        result.append(o_labels.clone())\n",
    "        #print (o_labels)\n",
    "        o_labels[:,:3] = o_labels[:,:3] > thres_all[model_number]\n",
    "        o_labels = ExtendLabel(o_labels)\n",
    "        prediction.append(o_labels)\n",
    "\n",
    "    prediction = torch.cat(prediction).detach().numpy().astype(int)\n",
    "    result = torch.cat(result).detach().numpy()\n",
    "    print (result)\n",
    "    import scipy.io\n",
    "    from datetime import datetime\n",
    "    now = datetime.now()\n",
    "    time = now.strftime(\"%D_%H_%M_%S\").replace('/','_')\n",
    "    scipy.io.savemat('Results_Training/{0}_{1}.mat'.format(valid_value, time), mdict={'result': result, 'prediction': prediction, 'best_weight': thres_all[model_number].tolist()})    \n",
    "#     torch.save(model.state_dict(), 'well-trained model/{0}_{1}.pkl'.format(valid_value, time))\n",
    "    torch.save(model, 'well-trained-model/{0}_{1}.pkl'.format(valid_value, time))\n",
    "    \n",
    "    SubmitGenerator(prediction, \n",
    "                    '../data/task2_sample_submission.csv',\n",
    "                    True, \n",
    "                    '../upload/{0}_{1}_task2_submission.csv'.format(valid_value, time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
