{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ipywidgets as widgets\n",
    "torch.manual_seed(17)\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('../data/task2_private_testset.csv', dtype=str)\n",
    "dataset.drop('Categories',axis=1,inplace=True)\n",
    "dataset.drop('Created Date',axis=1, inplace=True)\n",
    "dataset.drop('Authors',axis=1,inplace=True)\n",
    "dataset.to_csv('testset.csv',index=False)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For BERT\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def label_to_onehot(labels):\n",
    "    \"\"\" Convert label to onehot .\n",
    "        Args:\n",
    "            labels (string): sentence's labels.\n",
    "        Return:\n",
    "            outputs (onehot list): sentence's onehot label.\n",
    "    \"\"\"\n",
    "    label_dict = {'THEORETICAL': 0, 'ENGINEERING':1, 'EMPIRICAL':2, 'OTHERS':3}\n",
    "    onehot = [0,0,0,0]\n",
    "    for l in labels.split():\n",
    "        onehot[label_dict[l]] = 1\n",
    "    return onehot\n",
    "    \n",
    "def get_dataset(data_path, n_workers=4):\n",
    "    \"\"\" Load data and return dataset for training and validating.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the data.\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(data_path, dtype=str)\n",
    "\n",
    "    results = [None] * n_workers\n",
    "    with Pool(processes=n_workers) as pool:\n",
    "        for i in range(n_workers):\n",
    "            batch_start = (len(dataset) // n_workers) * i\n",
    "            if i == n_workers - 1:\n",
    "                batch_end = len(dataset)\n",
    "            else:\n",
    "                batch_end = (len(dataset) // n_workers) * (i + 1)\n",
    "            \n",
    "            batch = dataset[batch_start: batch_end]\n",
    "            results[i] = pool.apply_async(preprocess_samples, args=(batch,))\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    processed = []\n",
    "    for result in results:\n",
    "        processed += result.get()\n",
    "    return processed\n",
    "\n",
    "def preprocess_samples(dataset):\n",
    "    \"\"\" Worker function.\n",
    "\n",
    "    Args:\n",
    "        dataset (list of dict)\n",
    "    Returns:\n",
    "        list of processed dict.\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "    for sample in tqdm(dataset.iterrows(), total=len(dataset)):\n",
    "        processed.append(preprocess_sample(sample[1]))\n",
    "\n",
    "    return processed\n",
    "\n",
    "def preprocess_sample(data):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (dict)\n",
    "    Returns:\n",
    "        dict\n",
    "    \"\"\"\n",
    "    processed = {}\n",
    "#     processed['PaperId'] = int(data['PaperId'])\n",
    "    processed['Abstract'] = [data['Title'] + \".\"] + [sent for sent in data['Abstract'].split('$$$')]\n",
    "    #processed['Abstract'] = [sent for sent in data['Abstract'].split('$$$')]\n",
    "    #print (processed['Abstract'])\n",
    "    if 'Task 2' in data:\n",
    "        processed['Label'] = label_to_onehot(data['Task 2'])\n",
    "        \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[INFO] Start processing testset...')\n",
    "#test = get_dataset('testset.csv', embedder, n_workers=4)\n",
    "test = get_dataset('testset.csv', n_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from pytorch_transformers import *\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print (device)\n",
    "#bert-large-cased-whole-word-masking-finetuned-squad 1024\n",
    "#bert-base-uncased 768 \n",
    "#bert-large-uncased 1024\n",
    "#./scibert-scivocab-uncased/\n",
    "tokenizer = BertTokenizer.from_pretrained('../scibert_scivocab_uncased/')\n",
    "bert_model = BertModel.from_pretrained('../scibert_scivocab_uncased/')\n",
    "bert_model.to(device)\n",
    "bert_model.train(False)\n",
    "bert_model.eval()\n",
    "\n",
    "class BertDataset(Dataset):\n",
    "    def __init__(self, data, max_len = 256, n_workers=1):        \n",
    "        processed_data = []\n",
    "        for d in tqdm(data, total=len(data)):\n",
    "            processed_d = {'Abstract':[], 'Label':[]}\n",
    "            token_sent = None\n",
    "            for idx, sentence in enumerate(d['Abstract']):\n",
    "                if idx==0:\n",
    "                    token_sent = tokenizer.convert_tokens_to_ids(['[CLS]'] + tokenizer.tokenize(sentence) + ['[SEP]'])\n",
    "                else: \n",
    "                    token_sent += tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence) + ['[SEP]'])\n",
    "            if (len(token_sent) > 511):\n",
    "                token_sent = token_sent[:511] + tokenizer.convert_tokens_to_ids(['[SEP]'])\n",
    "            #print (len(token_sent))\n",
    "            encode_sentence_tensor = torch.tensor([token_sent])\n",
    "            encode_sentence_tensor = encode_sentence_tensor.to(device)\n",
    "            with torch.no_grad():\n",
    "                out = bert_model(encode_sentence_tensor)[0]\n",
    "                #print (out.shape)\n",
    "                out = out[:,-1,:]\n",
    "                #print (out.shape)\n",
    "            #processed_d['Abstract'] = [list(itertools.chain(out.to('cpu').tolist()[0], DocEmbed_32[d['PaperId']], DocEmbed_64[d['PaperId']], DocEmbed_128[d['PaperId']]))]\n",
    "            #processed_d['Abstract'] = [list(itertools.chain(out.to('cpu').tolist()[0], DocEmbed_64[d['PaperId']]))]\n",
    "            #print(DocEmbed_title_32[d['PaperId']].tolist())\n",
    "#             processed_d['Abstract'] = [list(itertools.chain(out.to('cpu').tolist()[0], DocEmbed_title_32[d['PaperId']].tolist(), CateEmbed_64[d['PaperId']].tolist()))]\n",
    "            processed_d['Abstract'] = [list(itertools.chain(out.to('cpu').tolist()[0]))]\n",
    "            #print(len(processed_d['Abstract'][0]))\n",
    "            #processed_d['Abstract'] = [out.to('cpu').tolist()[0]]\n",
    "            if 'Label' in d:\n",
    "                processed_d['Label'] = d['Label']\n",
    "            processed_data.append(processed_d)\n",
    "        \n",
    "        self.data = processed_data\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) # return data筆數\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "     \n",
    "    \n",
    "    def collate_fn(self, datas):\n",
    "        # get max length in this batch\n",
    "        max_sent = max([len(d['Abstract']) for d in datas])# Get max length of sentence in datas\n",
    "        batch_abstract = None\n",
    "        batch_label = []\n",
    "        for idx, data in enumerate(datas):\n",
    "            # padding abstract to make them in same length\n",
    "            #pad_abstract = data['Abstract']\n",
    "            if idx==0:\n",
    "                batch_abstract = data['Abstract']\n",
    "            else: \n",
    "                batch_abstract += data['Abstract']\n",
    "            #print (len(batch_abstract))\n",
    "            # gather labels\n",
    "            if 'Label' in data:\n",
    "                batch_label.append(data['Label'])\n",
    "        #print (batch_abstract)\n",
    "        #print (batch_label)\n",
    "        return torch.FloatTensor(batch_abstract), torch.FloatTensor(batch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = BertDataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class simpleNet(nn.Module):\n",
    "    def __init__(self, vocabulary_size):\n",
    "        super(simpleNet, self).__init__()\n",
    "        self.hidden_dim1 = 512\n",
    "        self.l0 = nn.Linear((vocabulary_size), (vocabulary_size))\n",
    "        self.b0 = nn.Parameter(torch.zeros(vocabulary_size))\n",
    "        self.l1 = nn.Linear((vocabulary_size), self.hidden_dim1)\n",
    "        self.b1 = nn.Parameter(torch.zeros(self.hidden_dim1))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.l2 = nn.Linear(self.hidden_dim1, 4)\n",
    "        self.b2 = nn.Parameter(torch.zeros(4))\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "        self.conv1 = nn.Conv1d(in_channels=1,out_channels=5,kernel_size=(3,))\n",
    "        self.b3 = nn.Parameter(torch.zeros(5, (vocabulary_size-2)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b,e = x.shape\n",
    "        x0 = self.relu1(self.l0(x)+self.b0)\n",
    "        x0 = self.dropout(x0)\n",
    "        x = self.relu1(self.l1(x+x0)+self.b1)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.l2(x)+self.b2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange\n",
    "import scipy.io\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SubmitGenerator(prediction, sampleFile, public=True, filename='prediction.csv'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        prediction (numpy array)\n",
    "        sampleFile (str)\n",
    "        public (boolean)\n",
    "        filename (str)\n",
    "    \"\"\"\n",
    "    sample = pd.read_csv(sampleFile)\n",
    "    submit = {}\n",
    "    submit['order_id'] = list(sample.order_id.values)\n",
    "    redundant = len(sample) - prediction.shape[0]\n",
    "    if public:\n",
    "        submit['THEORETICAL'] = list(prediction[:,0]) + [0]*redundant\n",
    "        submit['ENGINEERING'] = list(prediction[:,1]) + [0]*redundant\n",
    "        submit['EMPIRICAL'] = list(prediction[:,2]) + [0]*redundant\n",
    "        submit['OTHERS'] = list(prediction[:,3]) + [0]*redundant\n",
    "    else:\n",
    "        submit['THEORETICAL'] = [0]*redundant + list(prediction[:,0])\n",
    "        submit['ENGINEERING'] = [0]*redundant + list(prediction[:,1])\n",
    "        submit['EMPIRICAL'] = [0]*redundant + list(prediction[:,2])\n",
    "        submit['OTHERS'] = [0]*redundant + list(prediction[:,3])\n",
    "    df = pd.DataFrame.from_dict(submit) \n",
    "    df.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print (device)\n",
    "dataloader = DataLoader(dataset=testData,\n",
    "                            batch_size=64,\n",
    "                            shuffle=False,\n",
    "                            collate_fn=testData.collate_fn,\n",
    "                            num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cnt = 0\n",
    "for f in os.listdir('./well-trained-model'):\n",
    "    cnt += 1\n",
    "    print(f)\n",
    "    model = torch.load('./well-trained-model/'+f)\n",
    "    d = scipy.io.loadmat('./Results/'+f[:-4]+'.mat')\n",
    "    \n",
    "    prediction = []\n",
    "    result = []\n",
    "    trange = tqdm(enumerate(dataloader), total=len(dataloader), desc='Predict')\n",
    "    for i, (x,y) in trange:\n",
    "        o_labels = model(x.to(device))\n",
    "        o_labels = o_labels.to('cpu').detach().numpy()\n",
    "        result.extend(o_labels.copy())\n",
    "        #print (o_labels)\n",
    "        o_labels[:,:3] = o_labels[:,:3] > d['best_weight']\n",
    "        o_labels = ExtendLabel(o_labels[:,:3])\n",
    "        prediction.extend(list(o_labels))\n",
    "#         print(len(prediction))\n",
    "\n",
    "    result = np.array(result)\n",
    "    prediction = np.array(prediction).astype(int)\n",
    "    scipy.io.savemat('Results_Testing/'+f[:-4]+'.mat', mdict={'result': result, 'prediction': prediction, 'best_weight': d['best_weight']})    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
